---
title: "Intro to R: Session 2"
output:
  html_notebook:
  pdf_document:
  html_document:
editor_options: 
  markdown: 
    wrap: sentence
---

# Chapter 2: Data Manipulation

## 1. Packages

This far, we've only covered so-called "base R functions" or "built-in functions", but R has an active community and sometimes further operations are needed, so we use **packages**.
These are including further functions, which we will use heavily in the following section.

### 1.1 The `tidyverse` package

One of the most influential and widely used package in R is the `tidyverse` package.
This package includes several other packages, which are key for data manipulation e.g. `dplyr`, `ggplot2`, `stringr`, `readr`, `tidyr`.

tidyverse offers an alternative way of dealing with data as compared to Base R Commands.
Both approaches have weaknesses and strengths.
This session was written before with a focus on tidyverse, but in QM you will be mainly learning Base R so I will focus on teaching you Base R as well, except in cases where it is much more efficient to use tidyverse.
I will leave the tidyverse code in the file for you to check out later.

## 2. Working with packages

### 2.1 Installing packages

To install packages you use the very creative `install.packages()` command in R.
Note that it is necessary to directly install a package in R.
This step is only required once.

```{r, eval=FALSE}

install.packages("tidyverse") #You need to put the name of the package into quotation marks 

```

### 2.2 Loading your packages

While you only need to install a package once, you need to load it every time in your script, when you open it.
You can do that with the `library()` function in R:

```{r}

library(tidyverse) #You run the code and voila you can use the package

```

It is always important to have an efficient workflow in R.
Traditional R users, load all packages they need at the beginning of their page.
Logically, so they just need to go back to the top of the script and need to load it every time they open the script.
But there are way more elegant and pragmatic ways to do that, and here is an example:

```{r setup,  message=FALSE, warning=FALSE, results='hide'}

# The next bit  is quite powerful and useful.
# First you define which packages you need for your analysis and assign it to
# the p_needed object.
p_needed <-
  c("tidyverse", "WDI")

# Now you check which packages are already installed on your computer.
# The function installed.packages() returns a vector with all the installed
# packages.
packages <- rownames(installed.packages())
# Then you check which of the packages you need are not installed on your
# computer yet. Essentially you compare the vector p_needed with the vector
# packages. The result of this comparison is assigned to p_to_install.
p_to_install <- p_needed[!(p_needed %in% packages)]
# If at least one element is in p_to_install you then install those missing
# packages.
if (length(p_to_install) > 0) {
  install.packages(p_to_install)
}
# Now that all packages are installed on the computer, you can load them for
# this project. Additionally the expression returns whether the packages were
# successfully loaded.
sapply(p_needed, require, character.only = TRUE)
```

## 3. Loading Data

### 3.1 Data types

The last step, before starting data manipulation is to read data into R.
There are different types of data structures, I will present you the most frequent ones and the codes to read them in:

| File          | File Extension | Command                                  |
|---------------|----------------|------------------------------------------|
| **Stata**     | .dta           | `read_dta()`                             |
| **CSV-Files** | .csv           | `read.csv()` (German csv: `read.csv2()`) |
| **RData**     | .RData;.rds    | `load()` , `read.rds()`                  |

### 3.2 The European Social Survey

For the following Data Manipulation Part, we will use the European Social Survey Round 10 with the topic "Democracy, Digital social contacts".
It is a high-quality survey conducted in 31 European countries.
Round 10 was conducted in 2020 and is the most recent ESS.
We will use it, since survey data is quite popular among students and further at some point everyone needs to work with it.
But do not worry if you do not like survey data, we will also cover other prominent Datasets.

You can freely download it via the [website](https://www.europeansocialsurvey.org/) of the ESS.
From there you need to go to the Data Portal and than you can download the Round you want, in the format you want.
As already mentioned, use the `.dta` or `.csv` format.

## 4.Loading the data

```{r}

d1 <- read.csv("data/ESS10_sub.csv") #If it is not in the same directory as the R-Project, you should specify the the so-called path. Put the path in quotation marks and assign it to an object to work with it easier.

#This could take some time, since the dataset is huge 

#You can see the loaded Dataset "d1" in the Environment 

```

### One last thing: Pipelines

The are two ways execute successive commands on the same object: 1) the Base R method is to place the command to be executed first in the center then add other commands on top of it, like an onion.
2) the tidyverse of "pipping" or sending the results from the first command into the next command.
let's look at an example:

```{r}

#Base R
q <- c(6,3,8)

sqrt(exp(mean(q)))  #first you get the mean, then exponential, then sqrt

#With a Pipe 

q %>%         #send q into the mean function
  mean() %>%  #send the result of the mean into the exp function
  exp() %>%   #send the result into the sqrt function
  sqrt()

#both of them are valid ways, just a matter of taste
#a rule of thumb is that tidyverse functions are more suited for piping

```

## 5. Data Manipulation

### 5.1 Filtering your data 

Often you are not interesting in using all your observations, rather want to select some rows based on a certain condition.
Remember in the last session, we selected certain elements from a vector using a logical condition.
We will follow the same procedure for dataframe.
let's have a quick reminder:

```{r filtering vectors}

#we have a vector for temperatures
temp <- c(20, 41, 10, -2 , 35, -3, 8, 18)

temp == 20 #find room temperature
room_temp <- temp[temp == 20]
print(room_temp)


temp > 30 #find very hot temperatures
hot_temp <- temp[temp > 30]
print(hot_temp)

```

Similarly for dataframes, we want to create a logical condition where it would be `TRUE` for the rows we want to select and `FALSE` for the rows we don't want to select.
Then this condition will be placed inside the [ ], for example `data_frame[condition, ]`.
Let's look at some code:



```{r filtering Base R, eval = FALSE}

#================================#
#Filtering for only one condition 
#================================#

d2 <- d1[d1$cntry == "HU", ]

#what we did here is telling are create a vector of TRUE and FALSE inside row side of the bracket, where it takes the value of TRUE if the country is "HU" (Hungary), and FALSE for all other countries. So we basically made all rows with the country Hungary TRUE and all other rows FALSE.

head(d2) #Lets have a look if everything worked out 


#let's try selecting "younger" people

d2 <- d1[d1$agea <= 40, ]

max(d2$agea) #Lets have a look if everything worked out


#================================#
#Filtering for multiple condition 
#================================#

d2 <- d1[d1$cntry == "HU" | d1$cntry == "FR", ]

#even better
d2 <- d1[d1$cntry %in% c("HU", "FR"), ]

unique(d2$cntry) #Lets have a look if everything worked out 


#let's make it even more complicated
d2 <- d1[d1$cntry %in% c("HU", "FR") & d1$agea <= 40, ]



#Lets have a look if everything worked out 
unique(d2$cntry)
max(d2$agea)

```


Your turn! Think about a friend and filter the dataset to only include people the same age AND gender AND country as them. The variable names are agea, gndr (1 male, 2 female), and cntry (use the unique(d1$cntry) command to see all the country names in the dataset). Find out the number of rows in the filtered dataset.

```{r}





```


Here is how filtering is done using tidyverse:

```{r filtering tidyverse, eval = FALSE}

#================================#
#Filtering for only one condition 
#================================#

d2 <- d1 %>% #Our dataset 
  filter(cntry == "HU") #filtering for cases only in Hungary. They use the iso2c code, thus two latters as a shortage. We assign it to a new object called d2 

head(d2) #Lets have a look if everything worked out 

d2 <- d1 %>%  
  filter(agea <= 40) #We only want participants younger than 40

head(d2) #Lets have a look if everything worked out


#================================#
#Filtering for multiple condition 
#================================#

d2 <- d1 %>% #Our dataset 
  filter(cntry %in% c("HU", "FR")) #filtering for cases in Hungary and France.

head(d2) #Lets have a look if everything worked out 

d2 <- d1 %>% #Our dataset 
  filter(cntry %in% c("HU", "FR") &
           agea <= 40) #filtering for cases under 40 in Hungary and France.

 #Lets have a look if everything worked out 

head(d2)

d2 <- d1 %>% #Our dataset
  filter(cntry %in% c("HU", "FR"), 
           agea <= 40) #filtering for cases under 40 in Hungary and France with a comma

head(d2)

```

### 5.2 Selecting columns

We obviously do not care about all variables a dataset can offer (mostly).
There are several ways to select the variables (columns) we need.
This of course depends on our research question.
Let's say we want to investigate if respondents (dis)trusting scientist are more likely to be willing to get vaccinated against COVID-19.
We want to control for Age, Education, Gender, and the Left-Right Position.

```{r selecting Base R}

d2 <- d1[c("trstsci", "getnvc19", "agea", "eisced", "gndr", "lrscale")]

#we create a new dataframe with only the variables with the given column names

head(d2) #always check

#Of course we can combine filtering and selecting 
d2 <- d1[d1$age <40 ,c("trstsci", "getnvc19", "agea", "eisced", "gndr", "lrscale")]

head(d2)

#We can also delete columns from dataset
#there are several ways to do this
colnames(d2)
#"trstsci"  "getnvc19" "agea"     "eisced"   "gndr"     "lrscale" 

#if we want to remove agea, we can put a -ve before their column number

d3 <- d2[c(-3)] #or just d2[-3]

colnames(d3) #it is gone now

#but this way is quick but is suboptimal because we "hardcode" the column number, and would be quite tedious if you have many columns

#the cleaner way of doing, using the which command
which(colnames(d2) == "agea")

#the which command finds the place (index) of the desired column
#now we can put this with a -ve in the bracket

d3 <- d2[-which(colnames(d2) == "agea")]
colnames(d3) #and it is gone!

#another way is as follows
(colnames(d2) == "agea") #is TRUE only where the column name is "agea"

#we put a ! (NOT) before it
!(colnames(d2) == "agea") #everything is TRUE except for "agea"

#let's put it inside the brackets:
d3 <- d2[!(colnames(d2) == "agea")]
colnames(d3) #same effect


#for multiple columns we use %in% instead of ==
d3 <- d2[-which(colnames(d2) %in% c("agea", "gndr"))] #or
d3 <- d2[!(colnames(d2) %in% c("agea", "gndr"))]

colnames(d3)


```


Now you try it out, check the column names of d1 (colnames(d1)), and check what the variables stand for in the code book in the data folder. Pick an interesting dependent variable for you, and select the variables that are relevant for that question. Try to create two dataframes: one with few variables and one with more controls.

```{r}






```



Here is how you select in tidyverse:

```{r}

d2 <- d1 %>%
   select(trstsci, getnvc19, agea, eisced, gndr, lrscale) #Let's get the for us relevant variables

head(d2) #always check

#Of course we can combine the commands 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(trstsci, getnvc19, agea, eisced, gndr, lrscale)

head(d2)

#We can also delete columns from dataset, lets take the d6 and delete `lrscale`

head(d2)

d3 <- d2 %>% 
  select(-lrscale) #We delete columns by simply putting a -ve before it


head(d2)


```



### 5.3 reordering your dataset

```{r}
#not usually useful, but you can do it if you want to using order()
order(c(6,5,8,7))

#it rearranges the index of each element so all elements would be in ascending order

order(c(6,5,8,7), decreasing = T) #now descending order

#let's plug this again in the [ ]

d3 <- d2[order(d2$agea),]

head(d2)
head(d3)

#now the dataframe is sorted by age

```


 Here is how you arrange in tidyverse:
```{r}

#Let us take the code for d7 as example again 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(trstsci, getnvc19, agea, eisced, gndr, lrscale) %>% 
  arrange(agea) #We just use the agea function and then R sorts the variable from the lowest to the highest 

head(d2)

#What if we want to have it from the highest to the lowest? 

#Let us take the code for d7 as example again 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(trstsci, getnvc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea))#We just use the desc() function in the arrange function and R sorts the variable from the lowest to the highest 

head(d2)

```

### 5.4 renaming and reordering columns

```{r}
#to rename a column, we use our friend which again
d3 <- d2

colnames(d3)

colnames(d3)[which(colnames(d3) == "agea")] <- "age"

colnames(d3) #you can change multiple names by using %in% instead of ==


#there is no easy way to reorder the columns and it has to be done manually

#either change the order of names or indices

#"trstsci"  "getnvc19" "age"      "eisced"   "gndr"     "lrscale" 

d3 <- d3[c("age", "gndr", "eisced", "lrscale", "trstsci", "getnvc19")]

colnames(d3)

#let's reset
d3 <- d2
d3 <- d3[c(3,5,4,6,1,2)]

colnames(d3)

```


Here is how to do it in tidyverse:

```{r}
#we again take d2s code 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(trstsci, getnvc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea)) %>%
  rename(get_vac = getnvc19, 
         age = agea, 
         education = eisced, 
         female = gndr) #First you put the new name, equal sign and then you put the old name of the variable 

head(d2)

#===============================#

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(trstsci, getnvc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea)) %>%
  rename(get_vac = getnvc19, 
         age = agea, 
         education = eisced, 
         female = gndr) %>% 
  relocate(lrscale, age, education, female, get_vac, trstsci) #determine the order

head(d2)


# We can also specifically determine where we want to have the variable 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(trstsci, getnvc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea)) %>%
  rename(get_vac = getnvc19, 
         age = agea, 
         education = eisced, 
         female = gndr) %>% 
  relocate(lrscale, .after = age) #after with a point in front of it puts the variable after age

head(d2)



d2 <- d1 %>%
  filter(agea < 40) %>%
  select(trstsci, getnvc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea)) %>%
  rename(get_vac = getnvc19, 
         age = agea, 
         education = eisced, 
         female = gndr) %>% 
  relocate(lrscale, .before = age) #before with a point in front of it puts the variable before age

head(d2)


```


### 5.5 Recoding and creating new variables


```{r}

#we already now how to create to variables

d2$age_sq <- d2$age^2  #age squared is a usual control term
```

There are several reasons to record a variables:

1) The current coding is counter intuitive 
Remember the example where political interest was coded from 1-4 and 1 meant very interested in politics?

2) you might want to convert a continuous variable into categorical
remember when we converted left-right ideology (0-10) into c("left", "center", "right)?

3) missing values are given a code that can confuse our calculations:

```{r}

max(d1$agea) #no on can be 999 years old
#this value in the code book means "Not Available"
#if we leave this value like this, R will think this respondent is 999 years old

#we need to recode this value into a missing value or NA in R

5 + NA

max(5,NA)

# R can't perform operations on missing values so it won't interfere with our regression

#we recode variables like we did before using ifelse

da <- d1 #making a copy of d1

da$agea <- ifelse(da$agea == 999, NA, da$agea)
max(da$agea) #answer is NA because now all the 999 values are NA's
max(da$agea, na.rm =T) #tells R to ignore NA's

#use the range() and table() functions to check whether you have unrealistic values

#let's recode gender into character
table(da$gndr)
da$gndr <- ifelse(da$gndr == 1, "Male", "Female")
table(da$gndr) #notice the difference?

#let's do a more complex recoding as a refresher
#recode education from 1-7 to "low", "medium", "high"
table(da$eisced) #should only take values from 1-7, all others should be NA

da$eisced <- ifelse(da$eisced < 3, "low",
                    ifelse(da$eisced < 5, "Medium",
                          ifelse(da$eisced <= 7, "high",NA)))

table(da$eisced) #looks good

#count missing data in a column?
sum(is.na(da$eisced))

is.na(c(1,2,3,NA,NA))
sum(is.na(c(1,2,3,NA,NA)))


```


Now we have learnt how to filter certain rows, select certain columns, rename columns and rearrange them, and recorde variables. Let's try to do a general exercise for all these steps.

Remember the research question and the variables you selected during the exercise for selecting columns (section 5.2).

In the code below, do the following steps:

1. create a new dataframe from d1, where you select the dependent variable, independent variable, and controls in this order (select at least 5 variables in total)
2. filter the dataframe to only include data for 3 countries of your choice and only people whose education level (eisced) is less than 5.
3. rename your variables into understandable names (avoid making spaces in the names, use underscores where necessary)
4. recode the variables to convert missing values (e.g. 999) into NA's
5. count how many NA's are in each column


```{r}









```



Here is different ways of doing recoding in tidyverse:

```{r tidyverse B}

d2 <- d1 %>%
  mutate(trstsci_10 = trstsci*10) #You use the mutate function by first declaring a new variable name, in our case trstsci_10 and then you write down your operation. 

head(d2)

#We can also combine two or more variables to create another variable and we can change the class of a variable

d2 <- d1 %>% 
  mutate(new_variable = trstsci*10/lrscale+67, 
         gndr_char = as.character(gndr))

head(d2)


#The recode() function can be used to convert a variable, this is particularly useful, if we have an empirical strategy, which requires that or if we want to make new variables with new classes out of it: 

d2 <- d1 %>% 
  mutate(
    gndr_fac = as.factor(gndr), #always check the class 
    lr_cat = recode(lrscale,
                    `0` = 0,
                    `1` = 0,
                    `2` = 0,
                    `3` = 0,
                    `4` = 0,
                    `5` = 1,
                    `6` = 2,
                    `7` = 2,
                    `8` = 2,
                    `9` = 2,
                   `10` = 2,
                   `77` = NA_real_, 
                   `88` = NA_real_,
                   `99` = NA_real_),
    gender = recode(gndr_fac, 
                    `1` = "Male", 
                    `2` = "Female"))

#first you tell R, what variable needs to be recoded, and then you can change the scaling by using the original scaling in single quotation marks and after the equal sign, you can put numeric values straight in and character values in double quotation marks.

#Let us check how it worked out 

table(d2$lr_cat)
table(d2$gender)


d2 <- d1 %>% 
  mutate(gndr = as.factor(gndr),
    lr_cat = case_when(
    lrscale < 5 ~ 0,
    lrscale == 5 ~ 1,
    lrscale > 5 ~ 2),
    gender = case_when(
    gndr == 1 ~ "Male",
    gndr == 2 ~ "Female"
    ))

#Case_when works like a generalized ifelse function, you use case_when(), decide a condition for the variable, and with a wave you decide for the new value, which you put into brackets

#Do not get confused with the brackets, I always do...

table(d2$gender)
table(d2$lr_cat)

d2 <- d1 %>% 
  mutate(gndr = as.factor(gndr),
         lr_cat = ifelse(lrscale < 5, 0, 
                            ifelse(lrscale == 5, 1, 
                                   ifelse(lrscale > 5, 2, NA
                                          ))),
         gender = ifelse(gndr == 1, "Male",
                         ifelse(gndr == 2, "Female", NA))
  )

#Check 

table(d2$lr_cat)
table(d2$gender)


```

## 6. Handling Missing Values/Incomplete Data

As you saw right now, not all data in a dataset is complete.
Of course not, there are several sources, which can lead to incomplete/missing data.
Which means we need to deal with it directly.
If we look into the codebook, the ESS declares different types of missing values with high numbers: 77 means "Refusal", so the respondent refused to answer, 88 means "dont know", and 99 "No answer".
Note that the ESS does so, since some researchers are interested in missing values, and why they happen, so they can investigate it.

For us, this is a problem, because we cannot run an analysis with missing values.
There are two options:

-   Using statistics to artificially fill them out, this called multiple imputation techniques

-   Just delete incomplete observations to have a dataset without missing values

We will do the second one, and we have seen already how to recode useless values to NAs so this should be clear by now.


```{r}
#For that we take our dataset d1 

d1 <- na.omit(d1) #deletes all rows with any column containing a missing value

example_df <- data.frame(v1 = c(1,2,3,NA), 
                         v2 = c("A", NA, "C", NA),
                         v3 = c(TRUE, TRUE, FALSE, NA))

print(example_df)

example_df <- na.omit(example_df)
print(example_df)




#in tidyverse
d1 <- d1 %>% 
  drop_na() 



```



Here is a summary of all the steps in tidyverse (the filtering,recoding..etc)
Now we have all ingredients to make our dataset.
Do you remember our research question?
We want to find out if people, who tend to not trust science are less willing to get vaccinated.
We want to do that for all people over 35.

```{r tidyverse A}

d2 <- d1 %>%
  filter(agea >=40) %>% #filtering for bigger equal 40 years old
  select(trstsci, getnvc19, agea, eisced, gndr, lrscale) %>% #selecting only relevant variables 
  arrange(desc(agea)) %>% #arranging them by age in a descending order 
  rename(no_vac = getnvc19, #renaming variables with unintuitive names 
         age = agea, 
         education = eisced, 
         female = gndr) %>% 
  mutate( #Coding the missing categories into actual missing values 
    no_vac = case_when( 
      no_vac == 1 ~ 0, 
      no_vac == 2 ~ 1, 
      TRUE ~ no_vac),
    trstsci = case_when(
      trstsci %in% c(77, 88, 99) ~ NA_real_, 
      TRUE ~ trstsci), 
    age = case_when(
      age == 999 ~ NA_real_,
      TRUE ~ age), 
    education = case_when(
      education %in% c(55, 77, 88, 99) ~ NA_real_,
      TRUE ~ education), 
    female = case_when(
      female == 1 ~ 0, 
      female == 2 ~ 1, 
      female == 9 ~ NA_real_, 
      TRUE ~ female),
    lrscale = case_when(
      lrscale %in% c(77, 88, 99) ~ NA_real_,
      TRUE ~ lrscale)
    ) %>% 
  drop_na() #dropping all rows with NAs

head(d2)

```

## 7. Merging Datasets

Sometimes it could be the case that you need variables, which are not in one dataset already available, but in another dataset.
For this case you load both datasets and **merge** them together.
**This only works if there is a similar data structure, so know your data !**

As an example, I will show how to do that with **World Bank Data**.
From this data we can gather nearly all important economic indicators.
But mostly we need to merge them to datasets we are interested in.
We will merge the **World Bank Data** with the **KOF Globalization Index**.
The **KOF Globalization Index** is one of the most prominent Globalization Indices, and highly influential.
It measures Globalization on different dimensions and differentiates between *de facto* and *de jure* Globalization.
Let's check out the corresponding article from Gygli et al. 2019.

There are several ways of getting World Bank Data, but I will show the most efficient.
There is the package `WDI` with which you can get data through an API (Application Programming Interface).
Long story, short we do not need to download anything and get the data directly:

First we define, which countries should be included.

```{r}

countries <- c("HU", "FR", "DE", "US")

```

Afterwards we define, which variables we want.
You do that by using the official indicator, thus the variable you want.
You can find the indicators on the website of the [world bank data](https://data.worldbank.org/indicator).
Click on the variables you want, then click on the details, and there you find the indicator.
I will use GDP per capita, Fuel exports, CO2 emissions (kt).

```{r}

indicators = c("NY.GDP.PCAP.CD", "TX.VAL.FUEL.ZS.UN", "EN.ATM.CO2E.KT")

```

Now we are ready to use the API.

```{r}


wb <- WDI( 
  country = countries, #We include our countries 
  indicator = indicators, #We include our variables 
  start = 1960, #start date 
  end = 2023) #end date 

#This takes some time, especially if you have more countries and indicators.

head(wb) #We get the countryname, the iso codes and our variables 

```

Let us get started with the merging process, we need the KOF Globalization Index data. Note that merging is much easier using tidyverse so we will be using it directly.

```{r}

kof <- read_dta("data/KOFGI_2022_public.dta")

#head(kof)

#glimpse(kof) #A nice function from dplyr


```

Since we do not need all variables we select the relevant ones.
For us it is country, year, and the KOF globalization Index.

```{r}

kof <- kof[c("code", "country", "year", "KOFGI")]

#kof <- kof %>%
 # select(code, country, year, KOFGI)

```

To merge data there are important functions from the **dplyr** package: The `left_join()` and the `inner_join()` function.

-   `left_join()`: You want to keep all observations in the first table, including matching observations in the second table.
    You merging the data from the right table to left table.

-   `inner_join()`: You want to keep only observations that match in both tables 


To merge two datasets, you need at least one common variable.
But most of the time you need two common identifiers.
Since the datasets are structured by country and year, we will use country and year.
Since there is only one country and year combination.
We want the KOF Globalization Index assigned to a country X at a year Y:

```{r}


#We want to merge into the World Bank Dataset 

merged_data <- left_join(wb, kof, by = c("iso3c" = "code", 
                                         "year" = "year")) #You define your first/left table, and then the second/right table comes next. Afterwards you define the common identifiers. 

head(merged_data) #Now we need to clean the dataset

#notice that we now have two columns called country.x and country.y?
#let's check the column names of the original datasets:
colnames(wb)
colnames(kof)

#both datasets have a column called country, so when merging the datasets you end up with two columns called country and that is not acceptable in R so the join command adds the .x and .y to distinguish the two columns.

#solution is to rename or remove one of the country columns
colnames(kof)[which(colnames(kof) == "country")] <- "cntry"


#let's merge again and see now:
merged_data_new <- left_join(wb, kof, by = c("iso3c" = "code", 
                                         "year" = "year"))

colnames(merged_data_new) #looks good!



#inner join: only keep common observations

merged_data2 <- inner_join(kof, wb, by = c("code" = "iso3c", 
                                           "year" = "year"))

head(merged_data2)

nrow(merged_data)
nrow(merged_data2) #inner_join has less rows!

```

## Exercise Section:

### Exercise 1:

You are interested in discrimination and the perception of the judicial.
More specifically, you want to know if people, who fell discriminated evaluate courts differently.
Below you see a table with all variables you want to include in your analysis:

| **Variable**    | **Description**                                   |
|-----------------|---------------------------------------------------|
| **cntry**       | Country of respondent                             |
| **dscrgrp**     | Member of group discriminated against in country  |
| **cttresa**     | The courts treat everyone the same                |
| **agea**        | Age                                               |
| **gndr**        | Gender                                            |
| **eisced**      | Highest level of education                        |
| **lrscale**     | Left_Right Placement                              |
| --------------- | ------------------------------------------------- |

a.  Wrangle the data, and assign it to an object called **ess**.
b.  Select the variables you need
c.  Filter for Austria, Belgium, Denmark, Georgia, Iceland and the Russian Federation
d.  Have a look at the codebook and code all irrelevant values as missing. If you have binary variables recode them from 1, 2 to 0 to 1\
e.  You want to build an extremism variable: You do so by subtracting 5 from the from the variable and squaring it afterwards. Call it extremism
f.  Rename the variables to more intuitive names, don't forget to name binary varaibles after the category which is on 1
g.  drop all missing values
h.  Check out your new dataset

```{r, eval=FALSE}

ess <- 

```

### Exercise 2: Merging Datasets

The `gapminder` package in R loads automatically the gapminder dataset.
gapminder is an independent educational non-profit fighting global misconceptions, check out their website: <https://www.gapminder.org/> The gapminder dataset is already loaded.

a.  Get an overview of the gapminder dataset. There are different ways to do so, you can choose by yourself

```{r, eval=FALSE}


```

b.  We already loaded World Bank Data in the object **wb**. Merge the two datasets! Use `left_join()` and merge the gapminder dataset into the World Bank data. Define a new object called mer_df

```{r}

#Check the structure of the dataset
head(wb)

```

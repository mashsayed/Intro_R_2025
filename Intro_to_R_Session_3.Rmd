---
title: "Intro to R: Session 3"
output:
  html_document:
    toc: TRUE
    toc_float: TRUE
  pdf_document:
    toc: TRUE
    toc_float: TRUE
  html_notebook:
    toc: TRUE
    code_folding: "none"
editor_options: 
  markdown: 
    wrap: sentence
---

# Chapter 3: Data Manipulation

## 1. Packages

So far, we have covered so-called “base R functions” or “built-in functions”, 
but R has an active community and sometimes further operations are needed.
We can use **packages** to expand R's functionality. Packages include additional 
functions, which we will use heavily in the following section.

### The `tidyverse` package

One of the most influential and widely used package in R is the `tidyverse` package.
This package includes several other packages, which are key for data manipulation 
e.g. `dplyr`, `ggplot2`, `stringr`, `readr`, `tidyr`. Installing `tidyverse` is
essentially a shortcut to installing all these other packages.

Tidyverse offers an alternative way of dealing with data compared to base R syntax.
Both approaches have strengths and weaknesses.
Originally, this session was written with a focus on tidyverse but in QM you will 
primarily learn base R so I will focus on teaching you base R as well, except 
in cases where it is much more efficient to use tidyverse. I will leave the 
tidyverse code in the file for you to check out later. In the knitted notebook,
you can easily switch between tabs showcasing base R and tidyverse syntax.

## 2. Working with packages

### 2.1 Installing packages

To install packages you use the very creative `install.packages()` command in R.
Note that it is necessary to directly install a package in R.
This step is only required once.

```{r, eval=FALSE}
install.packages("tidyverse") # Put the name of the package in "quotation marks" 

```

### 2.2 Loading your packages

While you only need to install a package once, you need to load it every time you 
open a script. You can do that with the `library()` function in R:

```{r}
library(tidyverse) # Load installed package (no "quotation marks" needed)

```

It is always important to have an efficient workflow in R.
Traditional R users load all packages they need at the beginning of their page.
They just need to go back to the top of the script and load it every time they 
open the script. This is sufficient for replication materials, where the entire 
script will be opened and executed. However, this approach is not automated and 
requires user action: R will display a notification and prompt the user to 
install required packages via a mouse click. 

There are way more elegant and pragmatic ways to do that (installing and loading 
packages) and here is an example:

```{r setup,  message=FALSE, warning=FALSE, results='hide'}
# The next code chunk is quite powerful and useful. First you define which 
# packages you need for your analysis and assign it to the p_needed object.
p_needed <-
  c("tidyverse", "WDI", "haven")

# Now you check which packages are already installed on your computer.
# The function installed.packages() returns a vector with all the installed
# packages.
packages <- rownames(installed.packages())
# Then you check which of the packages you need are not installed on your
# computer yet. Essentially you compare the vector p_needed with the vector
# packages. The result of this comparison is assigned to p_to_install.
p_to_install <- p_needed[!(p_needed %in% packages)]
# If at least one element is in p_to_install you then install those missing
# packages.
if (length(p_to_install) > 0) {
  install.packages(p_to_install)
}
# Now that all packages are installed on the computer, you can load them for
# this project. Additionally the expression returns whether the packages were
# successfully loaded.
sapply(p_needed, require, character.only = TRUE)
```

## 3. Loading Data

### 3.1 File types

The last step, before starting data manipulation is to read data into R.
There are different types of data structures, I will present you the most frequent
ones and the codes to read them in:

| File          | File Extension | Command                                  |
|---------------|----------------|------------------------------------------|
| **CSV-Files** | .csv           | `read.csv()` (German csv: `read.csv2()`) |
| **Stata**     | .dta           | `read_dta()`                             |
| **RData**     | .RData;.rds    | `load()` , `read.rds()`                  |

By default, `read.csv()` identifies commas "," as seperators (to seperate data entries),
whereas `read.csv2()` identifies semicolons ";" as seperators.
An alternative to using `read.csv2()` is to explicitly declare the seperator using
`read.csv(file, sep = ";")`. For further information, consult the documentation 
in R using the command `?read.csv`.

### 3.2 Example: The European Social Survey

For the following Data manipulation part, we will use the European Social Survey 
(ESS) Round 11, conducted from 2023 to 2024 in 31 European countries with the 
topic “Social inequalities in health, Gender in contemporary Europe". As survey 
data is quite popular among students and common in the social sciences, the ESS, 
a high-quality survey, serves as a great example. Do not worry if you do not like
survey data, we will cover other prominent datasets, too.

You can freely download ESS data via their [website](https://www.europeansocialsurvey.org/) 
(requires registration and login). Go to the Data Portal and select the round you
want, in the format you want (preferably `.csv` or `.dta`). Every round of the 
ESS contains responses on a fixed set of questions part of every round, and 
additionally for a set of questions on two or three specific topics. For example, 
questions on the topic of “Social inequalities in health” are available in Round
11 (2023-2024) and Round 7 (2014-2015). Questions on “Gender in contemporary 
Europe" were posed for the first time in Round 11.

### 3.3. Loading the data

```{r}
# By default, the location of the R-project (.Rproj) or script (.R/.Rmd) is also
# the working directory (wd). 

# If the file is not in the same directory you should specify the path where R 
# can find the file. It is recommended to declare relative paths, a path relative
# to the working directory. To declare the path include the folders and the file 
# name in quotation marks. For example:
d1 <- read.csv("data/ESS11.csv") # assign the imported data file to an object

# Nowadays, using single forward slashes '/' works for all common operating 
# systems, incl. Windows and Unix-based systems (macOS, Linux).

# You can use "../" if the file is located in the parent directory of the wd.
# For example: "../ESS11.csv"


# Give it a moment to load the 70MB+ of data... 
# and See the loaded dataset "d1" in the 'Environment'
head(d1) # Preview of the first entries (6 by default)

```

## 4. Data Manipulation

### 4.0 Before we continue: Pipelines {.tabset .tabset-fade .tabset-pills}

The are two distinct ways to execute successive commands on the same object:

1.    The original base R method is to place the command to be executed first in 
the center then add other commands on top of it, like an onion: The code is read 
inverse to the order by which the functions are applied: sqrt() is applied last, 
mean() is applied first.

2.1.  Base R pipes '\|\>' or 
2.2   `margrittr` pipes '%\>%'.
Pipes in R were originally introducedby the 'magrittr' package (included in dplyr
and consequently in tidyverse too). In version 4.1.0, R introduced its own native
version of pipes. With pipes, the code is written (and read) by its order of 
execution. Mean() is executed first, followed by exp() and sqrt() being executed
last.

####  Base R (original)

```{r}
q <- c(6,3,8)

sqrt(exp(mean(q)))  #first you get the mean, then exponential, then sqrt

```

####  Base R pipes

```{r}
q |>
  mean() |>
  exp() |>
  sqrt()

```

#### Margrittr pipes

```{r}
q %>%         #send q into the mean function
  mean() %>%  #send the result of the mean into the exp function
  exp() %>%   #send the result into the sqrt function
  sqrt()

```

####

Whether to use the original R coding logic or pipes is mostly a matter of taste.
Base R code (without pipes) can become cumbersome to read the more functions are
nested into another. 

As a rule of thumb, tidyverse functions are better suited for piping (with %\>%).
Base R pipes are slightly faster (more efficient) than margrittr pipes.

<div>When piping, the object or value on the left-hand side (in our example 'q') is 
passed as the first argument to the function [here 'mean()'] by default.
After the code is executed, the resulting value or object is again passed through
a pipe to the next function [here: exp()] and so.
To change this default behaviour, one can use placeholders ('\_' for \|\> and 
'.' for %\>%).</div>

### 4.1 Filtering your data {.tabset}

Often you are not interesting in using all your observations, rather want to 
select some rows based on a certain condition. Remember in the last session, we
selected certain elements from a vector using a logical condition. We will follow
the same procedure for dataframe. Let us have a quick reminder:

```{r filtering vectors}
# We have a vector of temperature values
temp <- c(20, 41, 10, -2 , 35, -3, 8, 18)

temp == 20 # find room temperature
room_temp <- temp[temp == 20]
print(room_temp)

temp > 30 # find very hot temperatures
hot_temp <- temp[temp > 30]
print(hot_temp)
```

Similarly for dataframes, we want to create a logical condition where it would be
`TRUE` for the rows we want to select and `FALSE` for the rows we do not want to 
select. Then this condition will be placed inside the [ ], for example 
`data_frame[condition, ]`. Let us look at some code:

#### base R

Here is how filtering is done using base R:

```{r filtering Base R, eval = FALSE}
#================================#
#Filtering for only one condition 
#================================#

d2 <- d1[d1$cntry == "HU", ]

#what we did here is telling are create a vector of TRUE and FALSE inside row side of the bracket, where it takes the value of TRUE if the country is "HU" (Hungary), and FALSE for all other countries. So we basically made all rows with the country Hungary TRUE and all other rows FALSE.

head(d2) #Lets have a look if everything worked out 


#let's try selecting "younger" people

d2 <- d1[d1$agea <= 40, ]

max(d2$agea) #Lets have a look if everything worked out


#================================#
#Filtering for multiple condition 
#================================#

d2 <- d1[d1$cntry == "HU" | d1$cntry == "FR", ]

#even better
d2 <- d1[d1$cntry %in% c("HU", "FR"), ]

unique(d2$cntry) #Lets have a look if everything worked out 


#let's make it even more complicated
d2 <- d1[d1$cntry %in% c("HU", "FR") & d1$agea <= 40, ]



#Lets have a look if everything worked out 
unique(d2$cntry)
max(d2$agea)

```

#### tidyverse

Here is how filtering is done using tidyverse:

```{r filtering tidyverse, eval = FALSE}

#================================#
#Filtering for only one condition 
#================================#

d2 <- d1 %>% #Our dataset 
  filter(cntry == "HU") #filtering for cases only in Hungary. They use the iso2c code, thus two latters as a shortage. We assign it to a new object called d2 

head(d2) #Lets have a look if everything worked out 

d2 <- d1 %>%  
  filter(agea <= 40) #We only want participants younger than 40

head(d2) #Lets have a look if everything worked out


#================================#
#Filtering for multiple condition 
#================================#

d2 <- d1 %>% #Our dataset 
  filter(cntry %in% c("HU", "FR")) #filtering for cases in Hungary and France.

head(d2) #Lets have a look if everything worked out 

d2 <- d1 %>% #Our dataset 
  filter(cntry %in% c("HU", "FR") &
           agea <= 40) #filtering for cases under 40 in Hungary and France.

 #Lets have a look if everything worked out 

head(d2)

d2 <- d1 %>% #Our dataset
  filter(cntry %in% c("HU", "FR"), 
           agea <= 40) #filtering for cases under 40 in Hungary and France with a comma

head(d2)

```

####

Your turn!

Think about a friend and filter the dataset to only include people the same age 
AND gender AND country as them. The variable names are agea, gndr (1 male, 
2 female), and cntry (use the unique(d1\$cntry) command to see all the country 
names in the dataset). Find out the number of rows in the filtered dataset.

```{r}





```

### 4.2 Selecting columns {.tabset}

We obviously do not care about all variables a dataset can offer (mostly).
There are several ways to select the variables (columns) we need.
This of course depends on our research question.
Let's say we want to investigate if respondents dis(trusting) "most people" were 
more likely to receive (government approved) COVID-19 vaccines.
We want to control for Age, Education, Gender, and the Left-Right Position.

#### base R

Here is how you select in base R:

```{r selecting base R}

d2 <- d1[c("ppltrst", "vacc19", "agea", "eisced", "gndr", "lrscale")]

#we create a new dataframe with only the variables with the given column names

head(d2) #always check
table(d2$vacc19) # codebook: (1) Yes, (2) No, (7)-(9) Missing Value

#Of course we can combine filtering and selecting 
d2 <- d1[d1$ppltrst < 3, 
         c("ppltrst", "vacc19", "agea", "eisced", "gndr", "lrscale")
      ]
head(d2)

# respondents below 40
d2 <- d1[d1$ppltrst < 3 & d1$agea < 40, 
         c("ppltrst", "vacc19", "agea", "eisced", "gndr", "lrscale")
      ]
head(d2)

# We can also delete columns from dataset
# there are several ways to do this
colnames(d2)
# "ppltrst"  "vacc19" "agea"     "eisced"   "gndr"     "lrscale" 




#if we want to remove agea, we can put a - before their column number

d3 <- d2[c(-3)] #or just d2[-3]

colnames(d3) #it is gone now

# but this way is quick but is suboptimal because we "hardcode" the column number,
# and would be quite tedious if you have many columns

#the cleaner way of doing, using the which command
which(colnames(d2) == "agea")

#the which command finds the place (index) of the desired column
#now we can put this with a -ve in the bracket

d3 <- d2[-which(colnames(d2) == "agea")]
colnames(d3) #and it is gone!

#another way is as follows
(colnames(d2) == "agea") #is TRUE only where the column name is "agea"

#we put a ! (NOT) before it
!(colnames(d2) == "agea") #everything is TRUE except for "agea"

#let's put it inside the brackets:
d3 <- d2[!(colnames(d2) == "agea")]
colnames(d3) #same effect


#for multiple columns we use %in% instead of ==
d3 <- d2[-which(colnames(d2) %in% c("agea", "gndr"))] #or
d3 <- d2[!(colnames(d2) %in% c("agea", "gndr"))]

colnames(d3)


```

#### tidyverse

Here is how you select in tidyverse:

```{r}

# Select the relevant variables we need
d2 <- d1 %>%
   select(ppltrst, vacc19, agea, eisced, gndr, lrscale)

head(d2) #always check

# Of course we can combine the commands 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(ppltrst, vacc19, agea, eisced, gndr, lrscale)

head(d2)

#We can also delete columns from dataset, lets take the d6 and delete `lrscale`

head(d2)

d3 <- d2 %>% 
  select(-lrscale) #We delete columns by simply putting a - before it


head(d2)


```

####

Now you try it out, check the column names of d1 (colnames(d1)), and check what 
the variables stand for in the code book in the data folder. Pick an interesting 
dependent variable for you, and select relevant variables to answer the question.

Try to create two dataframes: One only including key variables of interest 
and an one including additional control variables.

```{r}






```

### 4.3 reordering your dataset {.tabset}

#### base R

Here is how you arrange in base R:

```{r}
#not usually useful, but you can do it if you want to using order()
order(c(6,5,8,7))

#it rearranges the index of each element so all elements would be in ascending order

order(c(6,5,8,7), decreasing = T) #now descending order

#let's plug this again in the [ ]

d3 <- d2[order(d2$agea),]

head(d2)
head(d3)

#now the dataframe is sorted by age

```

#### tidyverse

Here is how you arrange in tidyverse:

```{r}

#Let us take the code for d7 as example again 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(ppltrst, vacc19, agea, eisced, gndr, lrscale) %>% 
  arrange(agea) # We just use the arrange function and then R sorts the variable
                # from the lowest to the highest value of agea

head(d2)

# What if we want to have it from the highest to the lowest? 

# Let us take the code for d7 as example again 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(ppltrst, vacc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea)) # We just use the desc() function, where desc means 
                      # descending in the arrange function and R sorts the 
                      # variable from the highest to the lowest value of agea

head(d2)

```

### 4.4 renaming and reordering columns {.tabset}

#### base R

Here is how to rename and reorder columns with base R syntax:

```{r}
#to rename a column, we use our friend which again
d3 <- d2

colnames(d3)

colnames(d3)[which(colnames(d3) == "agea")] <- "age"

colnames(d3) #you can change multiple names by using %in% instead of ==


#there is no easy way to reorder the columns and it has to be done manually

#either change the order of names or indices

#"trstsci"  "getnvc19" "age"      "eisced"   "gndr"     "lrscale" 

d3 <- d3[c("age", "gndr", "eisced", "lrscale", "ppltrst", "vacc19")]

colnames(d3)

#let's reset
d3 <- d2
d3 <- d3[c(3,5,4,6,1,2)]

colnames(d3)

```

#### tidyverse

Here is how to do it in tidyverse:

```{r}
#we again take d2s code 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(ppltrst, vacc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea)) %>%
  rename(get_vac = vacc19, 
         age = agea, 
         education = eisced, 
         female = gndr) #First you put the new name, equal sign and then you put the old name of the variable 

head(d2)

#===============================#

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(ppltrst, vacc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea)) %>%
  rename(get_vac = vacc19, 
         age = agea, 
         education = eisced, 
         female = gndr) %>% 
  relocate(lrscale, age, education, female, get_vac, ppltrst) #determine the order

head(d2)


# We can also specifically determine where we want to have the variable 

d2 <- d1 %>%
  filter(agea < 40) %>%
  select(ppltrst, vacc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea)) %>%
  rename(get_vac = vacc19, 
         age = agea, 
         education = eisced, 
         female = gndr) %>% 
  relocate(lrscale, .after = age) #after with a point in front of it puts the variable after age

head(d2)



d2 <- d1 %>%
  filter(agea < 40) %>%
  select(ppltrst, vacc19, agea, eisced, gndr, lrscale) %>% 
  arrange(desc(agea)) %>%
  rename(get_vac = vacc19, 
         age = agea, 
         education = eisced, 
         female = gndr) %>% 
  relocate(lrscale, .before = age) #before with a point in front of it puts the variable before age

head(d2)


```

### 4.5 Recoding and creating new variables {.tabset}

```{r}

#we already now how to create to variables

d2$age_sq <- d2$age^2  #age squared is a usual control term
```

There are several reasons to record a variables:

1.  The current coding is counter intuitive Remember the example where political interest was coded from 1-4 and 1 meant very interested in politics?

2.  you might want to convert a continuous variable into categorical remember when we converted left-right ideology (0-10) into c("left", "center", "right)?

3.  missing values are given a code that can confuse our calculations:

#### base R

Here are different ways of recoding with base R:

```{r}

max(d1$agea) #no on can be 999 years old
#this value in the code book means "Not Available"
#if we leave this value like this, R will think this respondent is 999 years old

#we need to recode this value into a missing value or NA in R

5 + NA

max(5,NA)

# R can't perform operations on missing values so it won't interfere with our regression

#we recode variables like we did before using ifelse

da <- d1 #making a copy of d1

da$agea <- ifelse(da$agea == 999, NA, da$agea)
max(da$agea) #answer is NA because now all the 999 values are NA's
max(da$agea, na.rm =T) #tells R to ignore NA's

#use the range() and table() functions to check whether you have unrealistic values

#let's recode gender into character
table(da$gndr)
da$gndr <- ifelse(da$gndr == 1, "Male", "Female")
table(da$gndr) #notice the difference?

#let's do a more complex recoding as a refresher
#recode education from 1-7 to "low", "medium", "high"
table(da$eisced) #should only take values from 1-7, all others should be NA

da$eisced <- ifelse(da$eisced < 3, "low",
                    ifelse(da$eisced < 5, "Medium",
                          ifelse(da$eisced <= 7, "high",NA)))

table(da$eisced) #looks good

#count missing data in a column?
sum(is.na(da$eisced))

is.na(c(1,2,3,NA,NA))
sum(is.na(c(1,2,3,NA,NA)))


```

#### tidyverse

Here are different ways of recoding with tidyverse:

```{r tidyverse B}

d2 <- d1 %>%
  mutate(ppltrst_10 = ppltrst*10) #You use the mutate function by first declaring a new variable name, in our case trstsci_10 and then you write down your operation. 

head(d2)

#We can also combine two or more variables to create another variable and we can change the class of a variable

d2 <- d1 %>% 
  mutate(new_variable = ppltrst*10/lrscale+67, 
         gndr_char = as.character(gndr))

head(d2)


#The recode() function can be used to convert a variable, this is particularly useful, if we have an empirical strategy, which requires that or if we want to make new variables with new classes out of it: 

d2 <- d1 %>% 
  mutate(
    gndr_fac = as.factor(gndr), #always check the class 
    lr_cat = recode(lrscale,
                    `0` = 0,
                    `1` = 0,
                    `2` = 0,
                    `3` = 0,
                    `4` = 0,
                    `5` = 1,
                    `6` = 2,
                    `7` = 2,
                    `8` = 2,
                    `9` = 2,
                   `10` = 2,
                   `77` = NA_real_, 
                   `88` = NA_real_,
                   `99` = NA_real_),
    gender = recode(gndr_fac, 
                    `1` = "Male", 
                    `2` = "Female"))

#first you tell R, what variable needs to be recoded, and then you can change the scaling by using the original scaling in single quotation marks and after the equal sign, you can put numeric values straight in and character values in double quotation marks.

#Let us check how it worked out 

table(d2$lr_cat)
table(d2$gender)


d2 <- d1 %>% 
  mutate(gndr = as.factor(gndr),
    lr_cat = case_when(
    lrscale < 5 ~ 0,
    lrscale == 5 ~ 1,
    lrscale > 5 ~ 2),
    gender = case_when(
    gndr == 1 ~ "Male",
    gndr == 2 ~ "Female"
    ))

#Case_when works like a generalized ifelse function, you use case_when(), decide a condition for the variable, and with a wave you decide for the new value, which you put into brackets

#Do not get confused with the brackets, I always do...

table(d2$gender)
table(d2$lr_cat)

d2 <- d1 %>% 
  mutate(gndr = as.factor(gndr),
         lr_cat = ifelse(lrscale < 5, 0, 
                            ifelse(lrscale == 5, 1, 
                                   ifelse(lrscale > 5, 2, NA
                                          ))),
         gender = ifelse(gndr == 1, "Male",
                         ifelse(gndr == 2, "Female", NA))
  )

#Check 

table(d2$lr_cat)
table(d2$gender)


```

####

Now we have learnt how to filter certain rows, select certain columns, rename columns and rearrange them, and recorde variables.
Let's try to do a general exercise for all these steps.

Remember the research question and the variables you selected during the exercise for selecting columns (section 5.2).

In the code below, do the following steps:

1.  create a new dataframe from d1, where you select the dependent variable, independent variable, and controls in this order (select at least 5 variables in total)
2.  filter the dataframe to only include data for 3 countries of your choice and only people whose education level (eisced) is less than 5.
3.  rename your variables into understandable names (avoid making spaces in the names, use underscores where necessary)
4.  recode the variables to convert missing values (e.g. 999) into NA's
5.  count how many NA's are in each column

```{r}









```


## 5. Handling Missing Values/Incomplete Data {.tabset}

As you saw right now, not all data in a dataset is complete.
Of course not, there are several sources, which can lead to incomplete/missing data.
Which means we need to deal with it directly.
If we look into the codebook, the ESS declares different types of missing values with high numbers: 77 means “Refusal”, so the respondent refused to answer, 88 means “dont know”, and 99 “No answer”.
Note that the ESS does so, since some researchers are interested in missing values, and why they happen, so they can investigate it.

For us, this is a problem, because we cannot run an analysis with missing values.
There are two options:

-   Using statistics to artificially fill them out, this called multiple imputation techniques

-   Just delete incomplete observations to have a dataset without missing values

### base R

We will do the second one, and we have seen already how to recode useless values 
to NAs so this should be clear by now.

```{r}
#For that we take our dataset d2

d2 <- na.omit(d1) #deletes all rows with any column containing a missing value

example_df <- data.frame(v1 = c(1,2,3,NA), 
                         v2 = c("A", NA, "C", NA),
                         v3 = c(TRUE, TRUE, FALSE, NA))

print(example_df)

example_df <- na.omit(example_df)
print(example_df)


```


### tidyverse

Here is a summary of all the steps in tidyverse (the filtering,recoding..etc) Now we have all ingredients to make our dataset.
Do you remember our research question?
We want to find out if people, who tend to not trust science are less willing to get vaccinated.
We want to do that for all people over 35.

```{r tidyverse A}

d2 <- d1 %>%
  filter(agea >=40) %>% #filtering for bigger equal 40 years old
  select(ppltrst, vacc19, agea, eisced, gndr, lrscale) %>% #selecting only relevant variables 
  arrange(desc(agea)) %>% #arranging them by age in a descending order 
  rename(no_vac = vacc19, #renaming variables with unintuitive names 
         age = agea, 
         education = eisced, 
         female = gndr) %>% 
  mutate( #Coding the missing categories into actual missing values 
    no_vac = case_when( 
      no_vac == 1 ~ 0, 
      no_vac == 2 ~ 1, 
      TRUE ~ no_vac),
    ppltrst = case_when(
      ppltrst %in% c(7, 8, 9) ~ NA_real_, 
      TRUE ~ ppltrst), 
    age = case_when(
      age == 999 ~ NA_real_,
      TRUE ~ age), 
    education = case_when(
      education %in% c(55, 77, 88, 99) ~ NA_real_,
      TRUE ~ education), 
    female = case_when(
      female == 1 ~ 0, 
      female == 2 ~ 1, 
      female == 9 ~ NA_real_, 
      TRUE ~ female),
    lrscale = case_when(
      lrscale %in% c(77, 88, 99) ~ NA_real_,
      TRUE ~ lrscale)
    ) %>% 
  drop_na() #dropping all rows with NAs

head(d2)

```

## 6. Merging Datasets

Sometimes it could be the case that you need variables, which are not in one dataset already available, but in another dataset.
For this case you load both datasets and **merge** them together.
**This only works if there is a similar data structure, so know your data !**

As an example, I will show how to do that with **World Bank Data**.
From this data we can gather nearly all important economic indicators.
But mostly we need to merge them to datasets we are interested in.
We will merge the **World Bank Data** with the **KOF Globalization Index**.
The **KOF Globalization Index** is one of the most prominent Globalization Indices, and highly influential.
It measures Globalization on different dimensions and differentiates between *de facto* and *de jure* Globalization.
Let's check out the corresponding article from Gygli et al. 2019.

There are several ways of getting World Bank Data, but I will show the most efficient.
There is the package `WDI` with which you can get data through an API (Application Programming Interface).
Long story, short we do not need to download anything and get the data directly:

First we define, which countries should be included.

```{r}

countries <- c("HU", "FR", "DE", "US")

```

Afterwards we define, which variables we want.
You do that by using the official indicator, thus the variable you want.
You can find the indicators on the website of the [world bank data](https://data.worldbank.org/indicator).
Click on the variables you want, then click on the details, and there you find the indicator.
I will use GDP per capita, Fuel exports, CO2 emissions (kt).

```{r}

indicators = c("NY.GDP.PCAP.CD", "TX.VAL.FUEL.ZS.UN", "EN.ATM.CO2E.KT")

```

Now we are ready to use the API.

```{r}


wb <- WDI( 
  country = countries, #We include our countries 
  indicator = indicators, #We include our variables 
  start = 1960, #start date 
  end = 2023) #end date 

#This takes some time, especially if you have more countries and indicators.

head(wb) #We get the countryname, the iso codes and our variables 

```

Let us get started with the merging process, we need the KOF Globalization Index data.
Note that merging is much easier using tidyverse so we will be using it directly.

```{r}

kof <- read_dta("data/KOFGI_2024_public.dta")

#head(kof)

#glimpse(kof) #A nice function from dplyr


```

Since we do not need all variables we select the relevant ones.
For us it is country, year, and the KOF globalization Index.

```{r}

kof <- kof[c("code", "country", "year", "KOFGI")]

#kof <- kof %>%
 # select(code, country, year, KOFGI)

```

To merge data there are important functions from the **dplyr** package: The `left_join()` and the `inner_join()` function.

-   `left_join()`: You want to keep all observations in the first table, including matching observations in the second table.
    You merging the data from the right table to left table.

-   `inner_join()`: You want to keep only observations that match in both tables

To merge two datasets, you need at least one common variable.
But most of the time you need two common identifiers.
Since the datasets are structured by country and year, we will use country and year.
Since there is only one country and year combination.
We want the KOF Globalization Index assigned to a country X at a year Y:

```{r}


#We want to merge into the World Bank Dataset 

merged_data <- left_join(wb, kof, by = c("iso3c" = "code", 
                                         "year" = "year")) #You define your first/left table, and then the second/right table comes next. Afterwards you define the common identifiers. 

head(merged_data) #Now we need to clean the dataset

#notice that we now have two columns called country.x and country.y?
#let's check the column names of the original datasets:
colnames(wb)
colnames(kof)

#both datasets have a column called country, so when merging the datasets you end up with two columns called country and that is not acceptable in R so the join command adds the .x and .y to distinguish the two columns.

#solution is to rename or remove one of the country columns
colnames(kof)[which(colnames(kof) == "country")] <- "cntry"


#let's merge again and see now:
merged_data_new <- left_join(wb, kof, by = c("iso3c" = "code", 
                                         "year" = "year"))

colnames(merged_data_new) #looks good!



#inner join: only keep common observations

merged_data2 <- inner_join(kof, wb, by = c("code" = "iso3c", 
                                           "year" = "year"))

head(merged_data2)

nrow(merged_data)
nrow(merged_data2) #inner_join has less rows!

```

## Exercise Section:

### Exercise 1:

You are interested in discrimination and the perception of the judicial.
More specifically, you want to know if people, who feel discriminated evaluate courts differently.
Below you see a table with all variables you want to include in your analysis:

| **Variable**    | **Description**                                   |
|-----------------|---------------------------------------------------|
| **cntry**       | Country of respondent                             |
| **dscrgrp**     | Member of group discriminated against in country  |
| **trstlgl**     | Trust in the legal system                         |
| **agea**        | Age                                               |
| **gndr**        | Gender                                            |
| **eisced**      | Highest level of education                        |
| **lrscale**     | Left_Right Placement                              |
| --------------- | ------------------------------------------------- |

a.  Wrangle the data, and assign it to an object called **ess**.
b.  Select the variables you need
c.  Filter for Austria, Belgium, Denmark, Georgia, Iceland and the Russian Federation
d.  Have a look at the codebook and code all irrelevant values as missing. 
    If you have binary variables recode them from 1, 2 to 0 to 1\
e.  You want to build an extremism variable: You do so by subtracting 5 from the
    from the variable and squaring it afterwards. Call it extremism
f.  Rename the variables to more intuitive names, don't forget to name binary 
    variables after the category which is on 1
g.  drop all missing values
h.  Check out your new dataset

```{r, eval=FALSE}

ess <- 

```

### Exercise 2: Merging Datasets

The `gapminder` package in R loads automatically the gapminder dataset.
gapminder is an independent educational non-profit fighting global misconceptions, 
check out their website: <https://www.gapminder.org/>. 
The gapminder dataset is already loaded.

a.  Get an overview of the gapminder dataset. There are different ways to do so, 
    you can choose by yourself

```{r, eval=FALSE}


```

b.  We already loaded World Bank Data in the object **wb**. 
    Merge the two datasets! Use `left_join()` from `tidyverse` or `merge()` with
    `all.x = TRUE` in base R syntax to merge the gapminder dataset into the World
    Bank data. Define a new object called `df_mer`.

```{r}
#Check the structure of the dataset
head(wb)

```

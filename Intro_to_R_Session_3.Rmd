---
title: "Intro to R: Session 3"
output:
  html_notebook:
  pdf_document:
  html_document:
editor_options: 
  markdown: 
    wrap: sentence
---


# Chapter 3: Exploratory Data Analysis

## 1. New Packages: The Psych-Package 

Initially developed for psychological research, the `psych` Package will prove helpful for us in political science. Next to data visualization, factor analysis, reliability analysis and correlation analysis, this package will also help us to get an overview of hour datasets that we're working with. Getting a general look at your data is essential for getting a deeper understanding of what you are working with.
We will also need the `tidyverse` and `haven`.

```{r packages, message=FALSE, warning=FALSE, results='hide'}

p_needed <-
  c("tidyverse", "psych", "ineq", "corrplot")

packages <- rownames(installed.packages())

p_to_install <- p_needed[!(p_needed %in% packages)]

if (length(p_to_install) > 0) {
  install.packages(p_to_install)
}

sapply(p_needed, require, character.only = TRUE)


```

## 2. Essential Datasets on Political Parties

Analyzing political parties, their positions and their election results is among the most essential tasks of political science. And to our luck, an abundance of this data has been and is collected by other scientists. To name a few, ParlGov, Manifesto Project, CHES, and POPPA are among the most commonly used datasets on said aspects of parties in (mainly) Europe. To get an understanding of exploratory data analysis, we will make use of the POPPA and the ParlGov datasets. 

- The POPPA data comprises an Expert Survey on European parties' positions on a number of different ideological dimensions, with a focus on measuring a party's populism. [Website](http://poppa-data.eu/) 

- ParlGov has collected data on a variety of countries, their elections, their outcome and the resulting cabinets. Generally, they provide an expansive data collection on elections and their results. [Website](https://www.parlgov.org/)

```{r data}
pop <- read_dta("data/party_means.dta")
parl <- read.csv("data/parlgov_election.csv")
```

### 2.1 Merging the Datasets

To get a complete dataset on party positions and the parties' vote shares, we will need to merge the two datasets. Do you remember how to do that? To our luck, it just so happens that the POPPA data includes a variable that has the ParlGov ID of every party, making merging easy for us. However, as POPPA only measures party positions at around 2018, we have to limit ParlGov to this time frame. 

```{r merge}
parl18 <- parl %>% 
  filter(election_date <= 2019 & election_date >= 2017) 

comp <- left_join(parl18, pop, by = c("party_id" = "parlgov_id"))
```

## 3. A First Look at the Data

Now that our dataset is complete, we have a dataset that includes the vote shares of parties as well as their political positions according to expert surveys. Time to get a good look at what data exactly we are dealing with! Let's make use of the `psych` package. 

```{r describe}
describe(comp)
```

The `describe()` function gives a great overview over each variable in the dataset. `Vars` simply shows us a count of the variables. Going to the last page, we can see that we have 42 variables in our dataset. `n` shows us the number of observations for each variable. Now, more interesting to us is the column named `mean`. This column shows us the arithmetic mean of each variable, something that of course only works with numeric variables. It shows us, that the average vote share lies at around 10.24%. However, this alone tells us relatively little. We also need the standard deviation (remember, the average deviation from the mean) as well. This is told by `sd`. We can see that a parties vote share lies at a mean of 10.24% with a standard deviation of +/- 10.98%, a rather big number in comparison to the mean. We can, from this, conclude that a mean will not get us very far, as the vote share seems to vary a lot. Three other important, that can show us how much the vote share actually varies, are `min`, `max`, and `range`, which tell us the minimum measured vote share, the highest measured vote share and the range between the minimum and maximum. Thus, we see that the lowest measured vote share is 0.21%, the highest is 55.04% and the range between the two is at 54.83% (max-min = range). We can of course also look at these values separately. The values are the same

```{r measure}
mean(comp$vote_share, na.rm = T) #For these functions to work, we will need to exclude missing values
sd(comp$vote_share, na.rm = T)
median(comp$vote_share, na.rm = T)
min(comp$vote_share, na.rm = T)
max(comp$vote_share, na.rm = T)
max(comp$vote_share, na.rm = T) - min(comp$vote_share, na.rm = T)

#Or everything in one command 

describe(comp$vote, na.rm = T)

```

### 3.1 A Look at Distributions

The normal way to do this is to plot the variables, but we can ignore that for now. We will do it later. 

### 3.2 Summary of Specific Variables

Perhaps you just want to look at a specific variable, you can simply use the following function.

```{r summary}
summary(comp$vote_share)
```

## 4. The Gini-Coefficient

Another commonly used measure, specifically when trying to judge if there is inequality in a distribution, is the Gini-Coefficient. Complete equality is imagined by a straight vertical line. The Lorenzcurve then portraits the actual distribution. Now, to measure the inequality of a distribution, the Gini-Coefficient, we estimate the size of the gap between these two lines. The closer the value gets to 1, the higher the inequality, while a value closer to 0 indicates a completely equal distribution. In R, we can easily estimate this as follows.

```{r gini}
Gini(comp$vote_share)
```
However, this becomes more interesting when comparing different countries.

```{r gini comp}
unique(comp$country_name) #Shows us all unique values in this variable!
neth <- comp %>% 
  filter(comp$country_name == "Netherlands")

uk <- comp %>% 
  filter(comp$country_name == "United Kingdom")

ger <- comp %>% 
  filter(comp$country_name == "Germany")

Gini(neth$vote_share)
Gini(uk$vote_share)
Gini(ger$vote_share)
```

As expected, the UK with it's majoritarian electoral system has the highest inequality when it comes to the distribution of vote shares among parties, while the countries with representative system lie at a gini of around 40%, which is lower than the general Gini we calculated before.


## 5. Frequency Tables

Another way to better understand the variables your dealing with and do some first and basic descriptive statistics, are frequency tables. The most common ones you will use and come across are normal frequency tables and proportional tables. Lets start with the first.

### 5.1 Standard Tables

The basic frequency table in R will show you the respective values or categories of a variable as well as how often it has been observed in the data. Thus, these tables make more sense for categorical variables. To start, let us look at the vote shares again. However, the tables work best on categorical variables, so let us create a categorical seat share variable first.

```{r freq}
comp$cat_vote <- cut(comp$vote_share,
                       breaks=c(0, 5, 10, 15, 20, 25, 50, 100),
                       labels=c('0-4','5-9', '10-14', '15-19',
                                '20-24', '25-49', '50+'))

table(comp$cat_vote)
```
We can now see that most parties have a vote share between 0% and 4%, and generally lower vote shares, while there are also a large number of parties that receive between 25 to 49 percent, likely in majoritarian systems. 
We can also create a table that cumulates the values from column to column.

```{r cumsum}
t1 <- table(comp$cat_vote)
cumsum(t1)
```
With this we can say: 77 parties in the data received between 0% and 4% of votes. 117 parties received 9% or less votes. 142 parties received 14% or less and so on... 


## 6. Crosstabulation

Now, let us get to something more interesting! We now learned a lot about how to get a first look at single variables in our datasets. However, usually in science, we care about how two or more variables are connected and possibly correlate. But before we get into extensive inferential statistics, here too, it can be helpful to get a first exploratory look at the data at hand! One way to do so is crosstabulation. Cross tables let us look at how the distribution of two variables together. Let's look at an example.

```{r cats}
comp <- comp %>% 
  group_by(country_name) %>% 
  mutate(seat_share = seats / sum(seats))

comp$cat_seat <- cut(comp$seat_share,
                       breaks=c(0, 0.1, 0.2, 0.3, 0.4, 
                                0.5, 0.6, 0.7, 0.8, 0.9, 1),
                       labels=c('0-9', '10-19', '20-29',
                                '30-39', '40-49', '50-59',
                                '60-69', '70-79',
                                '80-89', '90-100'))

t3 <- xtabs(~comp$cat_vote+comp$cat_seat)
t3
```

What can we see here? There appears to be a connection between the percentage of votes and the share of seats a party received. The higher the vote share, the higher the seat share as well! Honestly, not very surprising but hopefully it makes the advantage of cross-tables more understandable. Again, we can transform this into a proportional table.

## 7. Correlation Matrcies

We now found that vote share and seat share are correlated. Great. However, we can take this two steps further! We can calculate a correlation coefficient between the two variables with a simple command. The basic command calculates the Pearson R, but we can also tell the command to give us the Spearman Coefficient and Kendall's Tau-b.

```{r corr}
#We can only include observations without NA
cor(comp$seat_share, comp$vote_share, use = "complete.obs") 
cor(comp$seat_share, comp$vote_share, method = "spearman", use = "complete.obs")
cor(comp$seat_share, comp$vote_share, method = "kendall", use = "complete.obs")
```
All three coefficients show us that vote share and seat share are positively correlated. But this, again, is no surprise... How about we check the correlations of all (numeric) variables in our data to see if we do find a surprise? We can do so by calculating a correlation matrix. This matrix contains the correlation coefficients between all variables in the data, by simply inputting the dataset into the previous command. However, we have to exclude all variables that are not numeric.

## 8. Concluding Remarks

In this session we have learned how to get a first overview of the data we are dealing with. We know how to compute the common descriptive measures in statistics such as the mean, standard deviation, quantiles and more. We know how to get an impression of how the variables we are dealing with are distributed. We also have seen some statistical measures such as the Gini-Coefficient and correlation coefficients. Finally, we know how to compute correlation matrices and even visualize them in an appealing format. 

In the next session we will get more familiar with the process of visualizing your data with Basic R (and `ggplot2`) and make them look suitable for publications!

## Exercises

### Exercise 1: Power to the People

The main contribution of the POPPA data is it's measure of the parties' populist sentiments. Get an overview of this variable (`populism`). What is it's mean, median, and quantiles? How high is the IQR? And lastly, create a histogram with 40 bins and take a look at the variables distribution. 

```{r populi}

```

### Exercise 2: Anti-Immigration Sentiment, That's what "the People" want...?

Judging from our correlation plot, there appears to be a correlation between the opinion on immigration and how much a party claims to represent the "general will" of the citizens. Explore this correlation by computing the different correlation coefficients that we met today. How would you interpret this correlation?

```{r thepeop}

```




